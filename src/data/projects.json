{
  "projects": [
    {
      "id": "heart-disease",
      "title": "Sistema de Predicción de Enfermedades Cardíacas",
      "shortDescription": "Pipeline completo de machine learning para predecir riesgo cardíaco usando XGBoost, con despliegue en Docker y versionado de modelos",
      "image": "/project1.jpg",
      "tags": [
        "Python 3.10",
        "XGBoost",
        "Scikit-learn",
        "Pandas",
        "Joblib",
        "Docker",
        "Poetry"
      ],
      "links": {
        "demo": "/project/heart-disease",
        "github": "https://github.com/gillopy/Deployment_XGBoost_Heart_Disease_UCI",
        "pdf": "https://github.com/gillopy/Deployment_XGBoost_Heart_Disease_UCI",
        "ipynb": "https://github.com/gillopy/Deployment_XGBoost_Heart_Disease_UCI"
      },
      "fullDescription": "Implementación completa de machine learning con procesamiento robusto de datos y gestión de modelos. Características principales: - Pipeline automatizado de datos que maneja valores faltantes y escalado de características - División estratificada de datos con balanceo de clases - Seguimiento de rendimiento del modelo (Precisión, Recall, AUC-ROC) - Despliegue Dockerizado con gestión de dependencias Poetry - Versionado automático de modelos con serialización timestamp - Métricas de evaluación comprehensivas y reproductibilidad",
      "technicalSections": [
        {
          "title": "/src/data/data_loader.py",
          "code": " 1 | import pandas as pd\n 2 | \n 3 | def load_data(file_path: str) -> pd.DataFrame:\n 4 |     \"\"\"\n 5 |     Carga los datos desde un archivo CSV.\n 6 | \n 7 |     Args:\n 8 |         file_path (str): Ruta del archivo CSV.\n 9 | \n10 |     Returns:\n11 |         pd.DataFrame: DataFrame con los datos cargados.\n12 |     \"\"\"\n13 |     return pd.read_csv(file_path)\n14 | ",
          "image": "/project1-extra1.jpg"
        },
        {
          "title": "/src/data/data_processor.py",
          "code": " 1 | import pandas as pd\n 2 | import numpy as np\n 3 | \n 4 | def process_data(df: pd.DataFrame, columns_to_impute: list, target_column: str = None) -> tuple[pd.DataFrame, pd.Series]:\n 5 |     \"\"\"\n 6 |     Procesa los datos:\n 7 |     - Imputa los valores faltantes\n 8 |     - Escalar las variables numéricas\n 9 | \n10 |     Args:\n11 |        df (pd.DataFrame): DataFrame con los datos a procesar.\n12 |        columns_to_impute (list): Lista de columnas a imputar los valores faltantes.\n13 |        target_column (str, optional): Nombre de la columna objetivo.\n14 | \n15 |     Returns:\n16 |        pd.DataFrame: DataFrame con los datos procesados.\n17 |        pd.Series: Serie con la variable objetivo.\n18 |     \"\"\"\n19 |     # Reemplazar valores 0 por NaN en las columnas a imputar\n20 |     df[columns_to_impute] = df[columns_to_impute].replace(0, np.nan)\n21 |     \n22 |     # Extraer la columna objetivo si es proporcionada\n23 |     target = df[target_column] if target_column else None\n24 |     \n25 |     return df, target\n26 | ",
          "image": "/project1-extra2.jpg"
        },
        {
          "title": "/src/data/data_splitter.py",
          "code": " 1 | from sklearn.model_selection import train_test_split\n 2 | import pandas as pd\n 3 | \n 4 | def split_data(data: pd.DataFrame, target_column: str, test_size=0.2, random_state=42, stratify: bool = True) -> tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n 5 |     \"\"\"\n 6 |     Dividir los datos en conjuntos de entrenamiento y prueba\n 7 | \n 8 |     Args:\n 9 |         data (pd.DataFrame): Dataframe que contiene los datos a dividir\n10 |         target_column (str): Nombre de la columna objetivo\n11 |         test_size (float, optional): Proporción de los datos que quedan en el test. Defaults to 0.2.\n12 |         random_state (int, optional): Semilla para la aleatoriedad. Defaults to 42.\n13 | \n14 |     Returns:\n15 |         Tupla: Una tupla que contiene los conjuntos de entrenamiento y prueba\n16 |         - X_train (pd.DataFrame): Conjunto de entrenamiento de las variables independientes\n17 |         - X_test (pd.DataFrame): Conjunto de prueba de las variables independientes\n18 |         - y_train (pd.Series): Conjunto de entrenamiento de la variable objetivo\n19 |         - y_test (pd.Series): Conjunto de prueba de la variable objetivo\n20 |     \"\"\"\n21 |     X = data.drop(columns=target_column, axis=1)\n22 |     y = data[target_column]\n23 |     \n24 |     # Dividir el conjunto de datos en entrenamiento y prueba\n25 |     if stratify:\n26 |         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n27 |     else:\n28 |         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n29 |     \n30 |     return X_train, X_test, y_train, y_test\n31 | ",
          "image": null
        },
        {
          "title": "/src/main.py",
          "code": " 1 | import sys\n 2 | import os\n 3 | # Agregar la raíz del proyecto al PYTHONPATH\n 4 | sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n 5 | from src.data.data_loader import load_data\n 6 | from src.data.data_processor import process_data\n 7 | import pandas as pd\n 8 | from src.data.data_splitter import split_data\n 9 | from src.model.trainer import train_model\n10 | from src.model.evaluator import evaluate_model\n11 | from src.model.saver import save_model\n12 | \n13 | def main():\n14 |     \n15 |     # Cargar los datos\n16 |     heart_disease = load_data(file_path = \"data/raw/heart.csv\")\n17 |     \n18 |     # Preprocesar los datos\n19 |     processed_data, target = process_data(\n20 |                                   df=heart_disease, \n21 |                                   columns_to_impute=['trestbps', 'chol', 'thalach', 'oldpeak'],  # columnas que pueden tener NaN\n22 |                                   target_column='num'  # Cambio para coincidir con la columna objetivo del dataset\n23 |                                   )\n24 |     \n25 |     # Dividir los datos en conjuntos de entrenamiento y prueba\n26 |     X_train, X_test, y_train, y_test = split_data(processed_data, target_column='num')\n27 |     \n28 |     # Convertir 'y' en binario\n29 |     y_train_binary = (y_train > 0).astype(int)\n30 |     y_test_binary = (y_test > 0).astype(int)\n31 | \n32 |     # Entrenar el modelo\n33 |     model = train_model(X_train=X_train, y_train=y_train_binary)\n34 | \n35 | \n36 | \n37 |     # Evaluar el modelo\n38 |     accuracy, precision, recall, f1, auc = evaluate_model(model, test_data=X_test, y_test=y_test_binary)\n39 | \n40 |     # Imprimir las métricas\n41 |     print(f\"Accuracy: {accuracy:.2f}\")\n42 |     print(f\"Precision: {precision:.2f}\")\n43 |     print(f\"Recall: {recall:.2f}\")\n44 |     print(f\"F1: {f1:.2f}\")\n45 |     print(f\"AUC: {auc:.2f}\")\n46 | \n47 |     \n48 |     # Guardar el modelo\n49 |     save_model(model, model_path=\"models/trained_model\")\n50 |     \n51 | if __name__ == \"__main__\":\n52 |     main()\n53 | ",
          "image": null
        },
        {
          "title": "/src/model/evaluator.py",
          "code": " 1 | from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n 2 | import xgboost\n 3 | import pandas as pd\n 4 | \n 5 | def evaluate_model(model: xgboost.sklearn.XGBClassifier, test_data: pd.DataFrame, y_test: pd.Series) -> tuple[float, float, float, float, float]:\n 6 |     \"\"\"\n 7 |     Función para evaluar el modelo\n 8 |     \n 9 |     Args:\n10 |         model (xgboost.sklearn.XGBClassifier): Modelo a evaluar\n11 |         test_data (pd.DataFrame): Datos de prueba\n12 |         y_test (pd.Series): Etiquetas de prueba\n13 | \n14 |     Returns:\n15 |         tuple[float, float, float, float, float]: Accuracy, Precision, Recall, F1, AUC\n16 |     \"\"\"\n17 |     y_pred = model.predict(test_data)\n18 |     y_prob = model.predict_proba(test_data)[:, 1]\n19 |     \n20 |     accuracy = accuracy_score(y_test, y_pred)\n21 |     precision = precision_score(y_test, y_pred)\n22 |     recall = recall_score(y_test, y_pred)\n23 |     f1 = f1_score(y_test, y_pred)\n24 |     auc = roc_auc_score(y_test, y_prob)\n25 |     \n26 |     return accuracy, precision, recall, f1, auc\n27 | ",
          "image": "/project1-extra3.jpg"
        },
        {
          "title": "/src/model/saver.py",
          "code": " 1 | from datetime import datetime\n 2 | import joblib\n 3 | import xgboost\n 4 | \n 5 | def save_model(model: xgboost.sklearn.XGBClassifier, model_path: str) -> None:\n 6 |     \"\"\"\n 7 |     Guarda el modelo en un archivo usando joblib.\n 8 | \n 9 |     Args:\n10 |         model (xgboost.sklearn.XGBClassifier): Modelo a guardar.\n11 |         model_path (str): Ruta donde guardar el modelo (sin extensión).\n12 |     \"\"\"\n13 |     current_date = datetime.now().strftime(\"%Y-%m-%d\")\n14 |     full_path = f\"{model_path}_{current_date}.joblib\"\n15 |     joblib.dump(model, full_path)\n16 |     print(f\"Modelo guardado en {full_path}\")\n17 | ",
          "image": null
        },
        {
          "title": "/src/model/trainer.py",
          "code": " 1 | import xgboost as xgb\n 2 | import pandas as pd\n 3 | \n 4 | def train_model(X_train: pd.DataFrame, y_train: pd.Series, params: dict = None) -> xgb.sklearn.XGBClassifier:\n 5 |     \"\"\"\n 6 |     Función para entrenar el modelo\n 7 | \n 8 |     Args:\n 9 |         X_train (pd.DataFrame): Dataframe con las variables independientes\n10 |         y_train (pd.Series): Serie con la variable objetivo\n11 |         params (dict, optional): Diccionario con los parámetros del modelo. Defaults to None.\n12 | \n13 |     Returns:\n14 |         xgboost.sklearn.XGBClassifier: Modelo entrenado\n15 |     \"\"\"\n16 |     if params is None:\n17 |         params = {'objective': 'binary:logistic', \n18 |                   'eval_metric': 'logloss',\n19 |                   'n_estimators': 300, \n20 |                   'max_depth': 2, \n21 |                   'learning_rate': 0.1, \n22 |                   'random_state': 42}\n23 |     \n24 |     model = xgb.XGBClassifier(**params)\n25 |     model.fit(X_train, y_train)\n26 |     \n27 |     return model\n28 | ",
          "image": "/project1-extra4.jpg"
        }
      ]
    },

    {
      "id": "ande-analysis",
      "title": "Análisis de Demanda Eléctrica ANDE",
      "shortDescription": "Sistema de visualización, análisis y forecasting de la demanda eléctrica con comparativas históricas",
      "image": "/project2.jpg",
      "tags": [
        "Python 3.13",
        "Pandas",
        "Matplotlib",
        "Seaborn",
        "Openpyxl",
        "Pytest",
        "uv"
      ],
      "links": {
        "demo": "/project/ande-analysis",
        "github": "https://github.com/gillopy/demanda_de_potencia_max_ande_deployment_uv",
        "pdf": "https://github.com/gillopy/demanda_de_potencia_max_ande_deployment_uv",
        "ipynb": "https://github.com/gillopy/demanda_de_potencia_max_ande_deployment_uv"
      },
      "fullDescription": "Plataforma completa para análisis energético con:\n- Carga automatizada de datos desde Excel\n- Procesamiento estadístico de series temporales\n- Visualizaciones interactivas con marcadores estacionales\n- Sistema de pruebas automatizadas\n- Gestión de dependencias con uv\n- Generación de reportes gráficos profesionales\n- Forecasting de demanda eléctrica usando métodos tradicionales y modelos neuronales",
      "technicalSections": [
        {
          "title": "Ande_Demanda_de_Potencia_Mensual & Ande.ipynb",
          "code": "# Ande_Demanda_de_Potencia_Mensual\nDatos mensuales de la demanda de potencia máxima por mes, Paraguay.\n\n# Jupyter notebook converted to Python script.\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Leer el archivo Excel\ndf = pd.read_excel('demanda2.xlsx')\n\n# Seleccionar las columnas relevantes (Año y meses)\ncolumnas_meses = ['Año', 'Enero', 'Febrero', 'Marzo', 'Abril', 'Mayo', 'Junio', 'Julio', 'Agosto', 'Septiembre', 'Octubre', 'Noviembre', 'Diciembre']\ndf_meses = df[columnas_meses]\n\n# Reorganizar el DataFrame para ser compatible con Seaborn\ndf_meses_melted = pd.melt(df_meses, id_vars='Año', var_name='Mes', value_name='KWh')\n\n# Ordenar los meses correctamente\norden_meses = ['Enero', 'Febrero', 'Marzo', 'Abril', 'Mayo', 'Junio', 'Julio', 'Agosto', 'Septiembre', 'Octubre', 'Noviembre', 'Diciembre']\ndf_meses_melted['Mes'] = pd.Categorical(df_meses_melted['Mes'], categories=orden_meses, ordered=True)\n\n# Configurar tema y estilo de Seaborn\nsns.set_theme(style=\"white\")\n\n# Plot cada serie temporal de años en su propia faceta\ng = sns.relplot(\n    data=df_meses_melted,\n    x=\"Mes\", y=\"KWh\", col=\"Año\", hue=\"Año\",\n    kind=\"line\", palette=\"crest\", linewidth=4, zorder=5,\n    col_wrap=3, height=5, aspect=2, legend=False\n)\n\n# Personalizar cada subplot\nfor año, ax in g.axes_dict.items():\n    ax.text(0.9, 0.90, str(año), transform=ax.transAxes, fontweight=\"bold\", \n            bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.1'))\n    sns.lineplot(\n        data=df_meses_melted, x=\"Mes\", y=\"KWh\", units=\"Año\",\n        estimator=None, color=\".7\", linewidth=1, ax=ax\n    )\n    ax.tick_params(axis='x', rotation=90)\n    ax.set_xticks(range(len(orden_meses)))\n    ax.set_xticklabels(orden_meses)\n    ax.axvline(x=2, color='red', linestyle='--', linewidth=0.8, label='Marzo')\n    ax.axvline(x=8, color='red', linestyle='--', linewidth=0.8, label='Septiembre')\n    datos_año = df_meses_melted[df_meses_melted['Año'] == año]\n    promedio_anual = datos_año['KWh'].mean()\n    ax.plot(datos_año['Mes'], [promedio_anual] * len(datos_año), color='green', linestyle='--', linewidth=1, \n            label=f'Promedio Anual MW: {promedio_anual:.0f}')\n    for idx in range(len(orden_meses)):\n        ax.axvline(x=idx, color='black', linestyle='--', alpha=0.3, linewidth=0.2)\n    ax.legend()\n    ax.patch.set_edgecolor('black')\n    ax.patch.set_linewidth(1)\n    ax.set_xlim(-0.5, len(orden_meses) - 0.5)\n\ng.set_titles(\"\")\ng.set_axis_labels(\"\", \"MW\")\ng.tight_layout()\n\nplt.show()\ng.savefig(\"grafico_demandaactual.png\", dpi=200, bbox_inches=\"tight\")",
          "image": "/project2-extra1.jpg"
        },
        {
          "title": "Neuronal Forecasting",
          "code": "import logging\nimport pandas as pd\nfrom utilsforecast.plotting import plot_series\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NBEATS, NHITS\nfrom neuralforecast.utils import AirPassengersDF\n\nlogging.getLogger('pytorch_lightning').setLevel(logging.ERROR)\n\n# Cargar y transformar los datos\ndf2 = pd.read_excel('demanda2.xlsx')\n\nmeses_es_en = {\n    'Enero': 'January', 'Febrero': 'February', 'Marzo': 'March', 'Abril': 'April',\n    'Mayo': 'May', 'Junio': 'June', 'Julio': 'July', 'Agosto': 'August',\n    'Septiembre': 'September', 'Octubre': 'October', 'Noviembre': 'November', 'Diciembre': 'December'\n}\n\ndf_melted = df2.melt(id_vars=['Año'], var_name='Mes', value_name='y')\ndf_melted['Mes'] = df_melted['Mes'].map(meses_es_en)\ndf_melted['ds'] = pd.to_datetime(df_melted['Año'].astype(str) + '-' + df_melted['Mes'], format='%Y-%B')\ndf_final = df_melted[['ds', 'y']].sort_values('ds').reset_index(drop=True)\n\n# Agregar columna de identificador único\ndf_final['unique_id'] = 'ts_1'\n\n# Dividir datos en entrenamiento y prueba (últimas 12 entradas para prueba)\ntrain_df = df_final.iloc[:-12]\ntest_df = df_final.iloc[-12:]\n\nhorizonte = len(test_df)\n\n# Configurar y ajustar modelos NBEATS y NHITS\nmodelos = [\n    NBEATS(input_size=2 * horizonte, h=horizonte, max_steps=100, enable_progress_bar=False),\n    NHITS(input_size=2 * horizonte, h=horizonte, max_steps=100, enable_progress_bar=False)\n]\n\nnf = NeuralForecast(models=modelos, freq='ME')\nnf.fit(df=train_df)\npronostico_df = nf.predict()\n\n# Graficar la serie de entrenamiento y los pronósticos\nplot_series(train_df, pronostico_df)",
          "image": "/project2-extra2.jpg"
        },
        {
          "title": "StatsForecast Model",
          "code": "import pandas as pd\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\nimport matplotlib.pyplot as plt\n\n# Cargar y transformar los datos\ndf2 = pd.read_excel('demanda2.xlsx')\n\nmeses_es_en = {\n    'Enero': 'January', 'Febrero': 'February', 'Marzo': 'March', 'Abril': 'April',\n    'Mayo': 'May', 'Junio': 'June', 'Julio': 'July', 'Agosto': 'August',\n    'Septiembre': 'September', 'Octubre': 'October', 'Noviembre': 'November', 'Diciembre': 'December'\n}\n\ndf_melted = df2.melt(id_vars=['Año'], var_name='Mes', value_name='y')\ndf_melted['Mes'] = df_melted['Mes'].map(meses_es_en)\ndf_melted['ds'] = pd.to_datetime(df_melted['Año'].astype(str) + '-' + df_melted['Mes'], format='%Y-%B')\ndf_final = df_melted[['ds', 'y']].sort_values('ds').reset_index(drop=True)\n\ndf_final['unique_id'] = 1\n\nsf = StatsForecast(\n    models=[AutoARIMA(season_length=12)],\n    freq='ME'\n)\n\nsf.fit(df_final)\nforecast = sf.predict(h=12, level=[95])\n\nplt.figure(figsize=(10, 6))\nplt.plot(df_final['ds'], df_final['y'], label='Datos históricos')\nplt.plot(forecast['ds'], forecast['AutoARIMA'], label='Predicción', color='orange')\nplt.fill_between(forecast['ds'], forecast['AutoARIMA-lo-95'], forecast['AutoARIMA-hi-95'], color='orange', alpha=0.3, label='Intervalo de confianza 95%')\nplt.xlabel('Fecha')\nplt.ylabel('Demanda de Potencia')\nplt.title('Predicción de Demanda de Potencia Mensual')\nplt.legend()\nplt.show()",
          "image": "/project2-extra3.jpg"
        },
        {
          "title": "Prophet Model",
          "code": "import pandas as pd\nfrom prophet import Prophet\n# Cargar datos\ndf2 = pd.read_excel('demanda2.xlsx')\n\n# Diccionario para traducir los nombres de los meses de español a inglés\nmeses_es_en = {'Enero': 'January', 'Febrero': 'February', 'Marzo': 'March', 'Abril': 'April', 'Mayo': 'May', 'Junio': 'June', 'Julio': 'July', 'Agosto': 'August', 'Septiembre': 'September', 'Octubre': 'October', 'Noviembre': 'November', 'Diciembre': 'December'}\n\n# Convertir el DataFrame al formato deseado\ndf_melted = df2.melt(id_vars=['Año'], var_name='Mes', value_name='y')\n\n# Traducir los meses\ndf_melted['Mes'] = df_melted['Mes'].map(meses_es_en)\n\n# Crear la columna de fechas\ndf_melted['ds'] = pd.to_datetime(df_melted['Año'].astype(str) + '-' + df_melted['Mes'], format='%Y-%B')\n\n# Seleccionar solo las columnas necesarias\ndf_final = df_melted[['ds', 'y']]\n\n# Ordenar por fecha\ndf_final = df_final.sort_values(by='ds').reset_index(drop=True)\n\n# Mostrar resultado\nprint(df_final.head())\nm = Prophet()\nm.fit(df_final)\nfuture = m.make_future_dataframe(50, freq='MS')\nforecast = m.predict(future)\nfig = m.plot(forecast)\nm = Prophet(seasonality_mode='multiplicative')\nm.fit(df_final)\nforecast = m.predict(future)\nfig = m.plot(forecast)",
          "image": "/project2-extra4.jpg"
        }
      ]
    }
    
    ,
    {
      "id": "era5-climate",
      "title": "Análisis Climático Global con Datos ERA5",
      "shortDescription": "Visualización avanzada de datos meteorológicos 3D usando modelos ECMWF Copernicus",
      "image": "/project3.jpg",
      "tags": [
        "Python 3.11",
        "Xarray",
        "Cartopy",
        "NetCDF4",
        "Matplotlib",
        "GeoPandas",
        "cfgrib"
      ],
      "links": {
        "demo": "/project/era5-climate",
        "github": "https://github.com/gillopy/Era5data3D",
        "pdf": "https://github.com/gillopy/Era5data3D",
        "ipynb": "https://github.com/gillopy/Era5data3D"
      },
      "fullDescription": "Sistema completo para procesamiento y visualización de datos climáticos con:\n- Carga y procesamiento de formatos GRIB/NetCDF\n- Visualizaciones 2D/3D con proyecciones cartográficas\n- Análisis temporal de series climáticas\n- Integración con datos geoespaciales (GeoJSON)\n- Animaciones de evolución temporal\n- Modelado de temperaturas y variables atmosféricas",
      "technicalSections": [
        {
          "title": "Instalación de Dependencias",
          "code": "pip install cfgrib cartopy netCDF4 geopandas",
          "image": "/project3-extra1.jpg"
        },
        {
          "title": "Carga de Datos NetCDF4",
          "code": "import netCDF4 as nc\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\n\n# Ruta del archivo .nc (cambia esto por la ubicación de tu archivo)\nfile_path = \"/content/drive/MyDrive/3c7785ed8c7cf99c2ae80fafa694aef4.nc\"\n\n# Cargar el archivo NetCDF4\ndataset = nc.Dataset(file_path, mode=\"r\")\n\n# Obtener las variables necesarias\nt2m = dataset.variables['t2m'][0, :, :]\nlat = dataset.variables['latitude'][:]\nlon = dataset.variables['longitude'][:]\n\n# Convertir a Celsius\nt2m_celsius = t2m - 273.15\n\n# Crear la figura y el mapa\nfig, ax = plt.subplots(figsize=(10, 5), subplot_kw={'projection': ccrs.PlateCarree()})\nax.set_extent([lon.min(), lon.max(), lat.min(), lat.max()], crs=ccrs.PlateCarree())\n\n# Agregar características al mapa\nax.add_feature(cfeature.COASTLINE)\nax.add_feature(cfeature.BORDERS, linestyle=':')\nax.add_feature(cfeature.LAND, edgecolor='black')\nax.add_feature(cfeature.LAKES, edgecolor='black')\nax.add_feature(cfeature.RIVERS, edgecolor='blue')\n\n# Dibujar la temperatura\nc = ax.pcolormesh(lon, lat, t2m_celsius, cmap='coolwarm', transform=ccrs.PlateCarree())\n\n# Agregar barra de color\ncbar = plt.colorbar(c, orientation='horizontal', pad=0.05)\ncbar.set_label('2m Temperature (°C)')\n\n# Título del gráfico\nplt.title(\"2m Temperature from ECMWF\")\n\n# Mostrar el gráfico\nplt.show()\n\n# Cerrar el dataset\ndataset.close()",
          "image": "/project3-extra2.jpg"
        },
        {
          "title": "Visualización con Cartopy y GRIB",
          "code": "import numpy as np\nimport xarray as xr\nimport cfgrib\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport warnings\n\n# Ignorar advertencias específicas\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Ruta del archivo GRIB (modifica según tu ubicación)\nfile_path = \"/content/drive/MyDrive/53ff69f95521585163052af17eefd0c1.grib\"\n\n# Ruta del archivo GeoJSON (modifica según tu ubicación)\ngeojson_path = \"/content/drive/MyDrive/DEPARTAMENTOS_PY_CNPV2022.geojson.txt\"\n\n# Cargar el dataset GRIB\ndata = xr.open_dataset(file_path, engine='cfgrib')\n\n# Extraer temperatura y coordenadas\nlons = data.longitude.values\nlats = data.latitude.values\ntemp = data.stl2.values[0] - 273.15\n\n# Cargar el archivo GeoJSON\ngdf = gpd.read_file(geojson_path)\n\n# Crear la figura con una proyección ortográfica centrada en Sudamérica\nfig = plt.figure(figsize=(12, 12))\nax = fig.add_subplot(111, projection=ccrs.Orthographic(central_longitude=-60, central_latitude=-15))\n\n# Agregar características del mapa\nax.add_feature(cfeature.LAND, edgecolor='black')\nax.add_feature(cfeature.OCEAN)\nax.add_feature(cfeature.COASTLINE)\nax.add_feature(cfeature.BORDERS, linestyle='-')\n\n# Dibujar el límite del mapa\nax.gridlines(draw_labels=False, linestyle=\"--\", color=\"gray\", alpha=0.5)\n\n# Proyectar los datos de temperatura\nvmin, vmax = -40, 40\nmesh = ax.imshow(temp, extent=[lons.min(), lons.max(), lats.min(), lats.max()], transform=ccrs.PlateCarree(), cmap='coolwarm', origin='upper', vmin=vmin, vmax=vmax)\n\n# Agregar barra de colores\ncbar = plt.colorbar(mesh, orientation='horizontal', pad=0.05, extend='both')\ncbar.set_label('Temperature (°C)')\ncbar.set_ticks(np.arange(vmin, vmax + 10, 10))\n\n# Agregar título\nplt.title('ERA5-Pressure-levels, Daily Mean Temperature at 1000 hPa\\n2025-02-07 (UTC)')\n\n# Mostrar el gráfico\nplt.show()",
          "image": "/project3-extra3.jpg"
        },
        {
          "title": "Visualización con Cartopy y GeoJSON",
          "code": "import numpy as np\nimport xarray as xr\nimport cfgrib\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport warnings\n\n# Ignorar advertencias específicas\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Ruta del archivo GRIB (modifica según tu ubicación)\nfile_path = \"/content/drive/MyDrive/53ff69f95521585163052af17eefd0c1.grib\"\n\n# Ruta del archivo GeoJSON (modifica según tu ubicación)\ngeojson_path = \"/content/drive/MyDrive/DEPARTAMENTOS_PY_CNPV2022.geojson.txt\"\n\n# Cargar el dataset GRIB\ndata = xr.open_dataset(file_path, engine='cfgrib')\n\n# Extraer temperatura y coordenadas\nlons = data.longitude.values\nlats = data.latitude.values\ntemp = data.stl2.values[0] - 273.15\n\n# Cargar el archivo GeoJSON\ngdf = gpd.read_file(geojson_path)\n\n# Crear la figura con un zoom más adecuado\nfig = plt.figure(figsize=(15, 10))\nax = fig.add_subplot(111, projection=ccrs.PlateCarree())\n\n# Configurar la extensión del mapa para Sudamérica\nax.set_extent([-85, -30, -60, 15], crs=ccrs.PlateCarree())\n\n# Agregar características del mapa\nax.add_feature(cfeature.LAND, edgecolor='black')\nax.add_feature(cfeature.OCEAN)\nax.add_feature(cfeature.COASTLINE)\nax.add_feature(cfeature.BORDERS, linestyle='-')\n\n# Agregar límites de los departamentos desde el GeoJSON\ngdf.boundary.plot(ax=ax, edgecolor=\"black\", linewidth=1.2, transform=ccrs.PlateCarree())\n\n# Proyectar los datos con un rango de temperatura adecuado\nvmin, vmax = -40, 40\nmesh = ax.imshow(temp, extent=[lons.min(), lons.max(), lats.min(), lats.max()], transform=ccrs.PlateCarree(), cmap='coolwarm', origin='upper', vmin=vmin, vmax=vmax)\n\n# Agregar barra de colores\ncbar = plt.colorbar(mesh, orientation='horizontal', pad=0.05, extend='both')\ncbar.set_label('Temperature (°C)')\ncbar.set_ticks(np.arange(vmin, vmax + 10, 10))\n\n# Agregar título\nplt.title('ERA5-Pressure-levels, Daily Mean Temperature at 1000 hPa\\n2025-02-07 (UTC)')\n\n# Mostrar el gráfico\nplt.show()",
          "image": null
        }
      ]
    }
  ]
}
